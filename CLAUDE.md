为了给 Claude 或 Codex 提供清晰的指导，让他们实现一个 **FastLabel** 的 Python+HTML Demo（其中音频是一个应用，主要目标是通过标注来快速训练模型），我们需要一个详细的技术规格文档（spec）。下面是这个 **FastLabel** 项目的完整规范：

---

# **FastLabel Spec: Python+HTML Demo** (用于模型训练与主动学习标注)

## 1. **项目简介**

**目标**：创建一个 Python 后端（Flask）与前端（HTML）结合的 Demo，支持主动学习的标注流程。用户通过网页标注数据，系统基于已标注的数据进行增量训练（每标几张图后训练一次）。支持音频和图像（如 CIFAR-10）数据集的标注和训练，音频只是在其中一个应用场景，核心目标是**快速标注 + 快速训练**。

**主要应用场景**：

* **图像应用**：图像分类（如 CIFAR-10 数据集）。
* **音频应用**：音频分类（如噪声检测、语音识别、环境声音识别等）。

## 2. **技术栈**

* **后端**：Python 3 + Flask

  * 用于接收前端请求、处理模型训练、数据标注等。
  * 使用 `torch`（支持 MPS 设备）处理训练过程。
  * 使用 `open_clip`（CLIP）模型进行图像/音频特征提取。

* **前端**：HTML + JavaScript（前端交互）

  * 实现用户标注界面，实时展示未标注数据。
  * 用户可以选择标签，提交标注结果，前端通过异步请求与后端交互。

* **数据库/存储**：内存（缓存标注数据与模型特征）

  * 无需持久化数据库，使用内存存储标注的数据、模型特征缓存等。

## 3. **整体工作流程**

### **标注与训练流程**

1. **展示未标注数据**：
   系统从待标注数据池中选择一批数据（按不确定性或者聚类/多样性选择），展示给用户。用户需要对每个样本选择一个标签（例如：图像分类中的 0-9 类）。

2. **用户标注**：
   用户在前端界面选择每个未标注样本的标签，并提交标注结果。

3. **增量训练**：
   每次提交标注后，系统将这些标注结果添加到已标注数据集，并用这些数据训练一个简单的 MLP 分类头。

   * CLIP 模型的编码部分保持冻结（pretrained），只训练 MLP 部分。
   * 每标注一定数量的数据（如每 10 张），系统会触发一次训练。

4. **展示训练后的新批数据**：
   训练后，系统根据当前模型对未标注数据的预测结果更新并展示下一批待标注的数据。

### **选样策略**

* **不确定性选择**：基于当前模型对未标注数据的预测不确定性（例如，使用熵）。
* **聚类选择**：将未标注数据通过特征空间聚类，选择每个簇中的代表性样本进行标注。

### **用户界面（UI）交互**

* 网格展示图片/音频，并允许用户为每个数据项选择标签。
* 提供批量操作功能，用户可以一键提交所有标注的标签。
* 提供反馈显示已标注数据量和剩余未标注数据量。

## 4. **后端：功能实现与 API**

### **核心数据结构**

```python
@dataclass
class State:
    labeled: Dict[int, int]          # 已标注数据（索引 -> 标签）
    unlabeled: List[int]             # 未标注数据索引
    last_batch: List[int]            # 当前展示批次的索引
    embed_cache: Dict[int, np.ndarray]  # 缓存每个数据的特征嵌入（通过 CLIP）
    steps_since_train: int           # 距离上次训练的步骤
```

### **API 路由**

1. **`/api/next_batch`** - 获取下一批待标注数据：

   * 输入：无
   * 输出：当前批次的数据项（图像/音频及其预测标签）。

2. **`/api/submit_labels`** - 提交标注的标签：

   * 输入：包含标注结果的 JSON（例如，`[{"id": 1, "label": 3}, {"id": 2, "label": 5}]`）。
   * 输出：是否触发了训练，当前已标注数据量，剩余未标注数据量。

3. **训练过程**：

   * 每次训练使用已标注的数据进行增量训练，冻结 CLIP 的前向编码部分，只训练后续的 MLP 分类头。

### **训练模型**

* 使用 CLIP 提取图像/音频特征，并冻结 CLIP 编码器。
* 增量训练 MLP 分类头：

  * MLP 输入：CLIP 提取的特征嵌入。
  * MLP 输出：分类预测。

```python
class MLPHead(nn.Module):
    def __init__(self, input_dim: int, num_classes: int):
        super().__init__()
        self.fc = nn.Sequential(
            nn.Linear(input_dim, 512),
            nn.ReLU(),
            nn.Dropout(0.2),
            nn.Linear(512, num_classes)
        )

    def forward(self, x):
        return self.fc(x)
```

### **多线程与并发**

* 后端应支持多线程/异步操作，确保前端请求不会因模型训练阻塞。
* 使用锁（例如 `threading.Lock()`）保证多线程对内存状态的访问一致性。

## 5. **前端：用户交互**

### **页面结构**

* **顶部**：包含标注控制按钮（加载下一批数据、提交标注等）。
* **展示区域**：网格展示未标注数据（图像/音频）和对应的标签选择下拉框。
* **状态显示**：显示已标注数据量和剩余未标注数据量。

### **主要功能**

1. **加载下一批数据**：按钮触发后，前端请求后端 API 获取下一批待标注数据。
2. **提交标注数据**：用户对每个数据项进行标注后，提交这些标注结果到后端，后端触发增量训练。
3. **显示预测标签**：基于当前模型，显示每个样本的预测标签和置信度。

```html
<!-- 示例：展示一个数据项 -->
<div class="card">
    <img src="data:image/png;base64,{{ image_b64 }}" alt="image" />
    <div class="meta">ID: {{ id }}</div>
    <select>
        <option value="">选择标签...</option>
        <!-- 10 个标签 -->
        <option value="0">0 - label</option>
        <option value="1">1 - label</option>
        <!-- ... -->
    </select>
</div>
```

### **用户交互流**

1. 用户访问页面，点击“加载下一批数据”。
2. 后端返回 100 张未标注的图像，用户进行标注。
3. 用户提交标注，系统训练一个 MLP 分类头并返回下一批数据。
4. 循环此过程，系统根据标注数据的变化更新模型。

## 6. **数据与模型管理**

### **数据来源**

* **图像**：使用标准数据集（如 CIFAR-10、ImageNet）或自定义数据集。
* **音频**：使用标准音频数据集（如 ESC-50、UrbanSound8K）或自定义数据集。

### **模型保存与加载**

* 每次训练结束后，保存模型的参数，并允许后续加载继续训练。

## 7. **性能与可扩展性**

* **训练效率**：增量训练仅训练 MLP 分类头，冻结 CLIP 编码器，减少训练时间。
* **前端性能**：数据量较大时使用分页或批量加载，减少一次性加载的数据量。
* **后端性能**：合理使用内存缓存（如 CLIP 嵌入缓存）减少重复计算，提高速度。

## 补充
可能运行在 Apple Silicon / GPU 上，利用 MPS 或 cuda。
使用 celeba 为例子。https://zsc.github.io/widgets/celeba/48x48.png (宽 9600, 高 7200，每图为彩图 48x48 ，无填充)

---

### 总结

以上是 FastLabel 项目的详细技术规格文档，适用于实现图像和音频标注系统。通过该系统，用户可以通过标注数据来实时训练并更新模型，而无需等待长时间的训练过程。

